{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bcecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ca1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = pd.read_csv( 'bank.csv')\n",
    "bank_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6716717",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.subscribed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing resample from *sklearn.utils* package.\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the case of yes-subscribes and no-subscribes\n",
    "bank_subscribed_no = bank_df[bank_df.subscribed == 'no']\n",
    "bank_subscribed_yes = bank_df[bank_df.subscribed == 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Upsample the yes-subscribed cases.\n",
    "df_minority_upsampled = resample(bank_subscribed_yes,replace=True, n_samples=2000) #2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d1c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine majority class with upsampled minority class\n",
    "new_bank_df = pd.concat([bank_subscribed_no, df_minority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d315132",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_bank_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bank_df.subscribed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cbce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "new_bank_df = shuffle(new_bank_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a08a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_bank_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f85b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning list of all column names in the DataFrame\n",
    "X_features = list( new_bank_df.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the response variable from the list\n",
    "X_features.remove( 'subscribed' )\n",
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get_dummies() will convert all the columns with data type as objects\n",
    "encoded_bank_df = pd.get_dummies( new_bank_df[X_features], drop_first = True )\n",
    "X = encoded_bank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c75d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea53287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the subscribed column and assigning to Y\n",
    "Y = new_bank_df.subscribed.map( lambda x: int( x == 'yes') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split( X,Y,test_size = 0.3,random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### logistic regression\n",
    "## building the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## Initializing the model\n",
    "logit = LogisticRegression()\n",
    "## Fitting the model with X and Y values of the dataset\n",
    "logit.fit( train_X, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make prediction\n",
    "pred_y = logit.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfe9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "### predicting all the Y values for test_X\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ddc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predicint\n",
    "pred_single = logit.predict([[34,202,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1]])\n",
    "pred_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5bdfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564acd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the metrics\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the matrix to draw the confusion metrix from actual and predicted class labels\n",
    "def draw_cm( actual, predicted ):\n",
    "# Invoking confusion_matrix from metric package. The matrix will oriented as[1,0] i.e.\n",
    "# the classes with label 1 will be reprensted the first row and 0 as secondrow\n",
    "    cm = metrics.confusion_matrix( actual, predicted, [1,0] )\n",
    "    ## Confustion will be plotted as heatmap for better visualization\n",
    "    ## The lables are configured to better interpretation from the plot\n",
    "    sn.heatmap(cm, annot=True, fmt='.2f',\n",
    "    xticklabels = [\"Subscribed\", \"Not Subscribed\"] ,\n",
    "    yticklabels = [\"Subscribed\", \"Not Subscribed\"] )\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07620b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = draw_cm( test_y, pred_y )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ROC AUC Score\n",
    "\n",
    "## Predicting the probability values for test cases\n",
    "predict_proba_df = pd.DataFrame( logit.predict_proba( test_X ) )\n",
    "predict_proba_df.head()\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f29cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the DataFrame with actual class lables\n",
    "test_results_df = pd.DataFrame( { 'actual': test_y } )\n",
    "test_results_df = test_results_df.reset_index()\n",
    "## Assigning the probability values for class label 1\n",
    "test_results_df['chd_1'] = predict_proba_df.iloc[:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84382d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing actual class labels and the predicted probability values to compute ROC AUC score.\n",
    "auc_score = metrics.roc_auc_score( test_results_df.actual, test_results_df.chd_1)\n",
    "round( float( auc_score ), 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The method takes the three following parameters\n",
    "## model: the classification model\n",
    "## test_X: X features of the test set\n",
    "## test_y: actual labels of the test set\n",
    "## Returns\n",
    "## - ROC Auc Score\n",
    "## - FPR and TPRs for different threshold values\n",
    "def draw_roc_curve( model, test_X, test_y ):\n",
    "    ## Creating and initializing a results DataFrame with actual labels\n",
    "    test_results_df = pd.DataFrame( { 'actual': test_y } )\n",
    "    test_results_df = test_results_df.reset_index()\n",
    "    # predict the probabilities on the test set\n",
    "    predict_proba_df = pd.DataFrame( model.predict_proba( test_X ) )\n",
    "    ## selecting the probabilities that the test example belongs to class 1\n",
    "    test_results_df['chd_1'] = predict_proba_df.iloc[:,1:2]\n",
    "    ## Invoke roc_curve() to return the fpr, tpr and threshold values.\n",
    "    ## threshold values contain values from 0.0 to 1.0\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( test_results_df.actual,\n",
    "    test_results_df.chd_1,\n",
    "    drop_intermediate = False )\n",
    "    ## Getting the roc auc score by invoking metrics.roc_auc_score method\n",
    "    auc_score = metrics.roc_auc_score( test_results_df.actual, test_results_df.chd_1 )\n",
    "    ## Setting the size of the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ## plotting the actual fpr and tpr values\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    ## plotting th diagnoal line from (0,1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    ## Setting labels and titles\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return auc_score, fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637911cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Invoking draw_roc_curve with the logistic regresson model\n",
    "_, _, _, _ = draw_roc_curve( logit, test_X, test_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bfbde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
